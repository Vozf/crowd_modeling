{"cells":[{"cell_type":"markdown","metadata":{"id":"73KQfMTfpPh7"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLrFZCKppPh7"},"outputs":[],"source":["from tensorflow import keras\n","from keras.models import Model\n","from keras import Input\n","from keras.layers import Dense, Lambda, Concatenate, Reshape\n","from keras.utils import plot_model\n","from keras import backend as K\n","from tensorflow.python.keras.engine.keras_tensor import KerasTensor\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import History\n","import tensorflow_probability as tfp\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import graphviz\n","import plotly\n","import plotly.express as px\n","import sys\n","import os\n","from typing import List, Tuple\n","import tensorflow as tf\n","import random\n"]},{"cell_type":"code","source":["from google.colab import drive"],"metadata":{"id":"ZGnAtQyiqR5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gdrive_path='/content/gdrive/MyDrive/CM'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))"],"metadata":{"id":"Lxv6lSSlp9jf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4tamV0jBpPh8"},"source":["### Preprocessing & defining data"]},{"cell_type":"code","source":["from gensim.models import KeyedVectors"],"metadata":{"id":"WfCbWpplqr7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Specify the path to your Word2Vec dataset\n","# dataset_path = \"GoogleNews-vectors-negative300.bin.gz\"\n","\n","# # Load the Word2Vec model\n","# model = KeyedVectors.load_word2vec_format(dataset_path, binary=True)\n"],"metadata":{"id":"74BScRajqad6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_input(dataset_path: str) -> np.ndarray:\n","    model = KeyedVectors.load_word2vec_format(dataset_path, binary=True)\n","    all_vectors = model.vectors\n","    return all_vectors"],"metadata":{"id":"6xCxPt56rU03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"7IGLog8qzb0M"}},{"cell_type":"code","source":["# in between offer cut down of data\n","\n","cut_vectors = load_input(\"GoogleNews-vectors-negative300.bin.gz\")[0:100000]"],"metadata":{"id":"mMBQsyh6rFGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cut_vectors.shape"],"metadata":{"id":"GfG3LHPnrx7k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalise_abs_input(unnormalised_dataset: np.ndarray) -> np.ndarray:\n","    # Figure out the min value, so to make all vectors positive\n","    min_x_unormalised = min([min(i) for i in unnormalised_dataset])\n","    pos_x_train = unnormalised_dataset+abs(min_x_unormalised)\n","\n","    # Normalise the data by dividing by the max of the positive data\n","    max_pos_x = max([max(i) for i in pos_x_train])\n","    x_train = pos_x_train/max_pos_x\n","    return x_train"],"metadata":{"id":"M3Hs753Rs5w5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalized = normalise_abs_input(cut_vectors)"],"metadata":{"id":"IPM2e9kus7td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into train and test sets\n","x_train, x_test = train_test_split(normalized, test_size=0.2, random_state=42)\n","\n","# Split the train set into train and validation sets\n","x_train, x_val = train_test_split(x_train, test_size=0.1, random_state=42)"],"metadata":{"id":"BOF4wVPmukNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"id":"DKscvGSBtKsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test.shape"],"metadata":{"id":"obGh5iORvGNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_val.shape"],"metadata":{"id":"QhbQqzchvJMR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(x_train)"],"metadata":{"id":"D4QUme9quPlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of neurons at the input layer (28 * 28 = 784)\n","original_dim = 300\n","# latent space dimension\n","latent_dim = 2\n","# hidden layer dimension\n","hl_dim = 128"],"metadata":{"id":"UoYDmqv7tZui"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1YQEoSkpPh8"},"outputs":[],"source":["# Defining the input of the decoder\n","latent_inputs = Input(shape=(latent_dim,), name='Input_Z_Sampling')"]},{"cell_type":"code","source":["# Defining the input of the encoder\n","input = Input(shape=(original_dim,), name='Encoder_Input_Layer')"],"metadata":{"id":"DQJV57AHudig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kF4kHG_OpPh8"},"source":["### Functions"]},{"cell_type":"markdown","metadata":{"id":"vCBF8QA7pPh9"},"source":["1. Basic Functionalities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiHW4MSDpPh9"},"outputs":[],"source":["def sampling(args: Tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:\n","    \"\"\"\n","    Samples from a multivariate normal distribution using the reparameterisation trick.\n","    Args:\n","        args: A tuple of two tensors representing the mean and log standard deviation of the approximate posterior distribution.\n","    Returns:\n","        tf.Tensor: A tensor representing a sample drawn from the multivariate normal distribution.\n","    \"\"\"\n","    z_mean, z_log_sigma = args\n","    distribution = tfp.distributions.MultivariateNormalDiag(loc=z_mean, scale_diag=z_log_sigma)\n","    z = distribution.sample()\n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cetr8KcIpPh9"},"outputs":[],"source":["def create_encoder_model(input: KerasTensor, activation_mean: str = None, activation_sd: str = None,\n","                         hl_dim: int = 256, latent_dim: int = 2) -> Tuple[Model, KerasTensor, KerasTensor]:\n","    \"\"\"\n","    Creates an encoder model that samples from a multivariate normal distribution using the reparameterization trick.\n","\n","    Args:\n","        input (KerasTensor): The input tensor for the encoder model.\n","        activation_mean (str, optional): Activation function for the mean component of the latent space layer. Defaults to None.\n","        activation_sd (str, optional): Activation function for the standard deviation component of the latent space layer. Defaults to None.\n","        hl_dim (int, optional): Dimension of the hidden layers. Defaults to 256.\n","        latent_dim (int, optional): Dimension of the latent space. Defaults to 2.\n","\n","    Returns:\n","        Tuple[Model, tf.Tensor, tf.Tensor]: A tuple containing the encoder model, the tensor representing the mean component of the latent space,\n","        and the tensor representing the log standard deviation component of the latent space.\n","    \"\"\"\n","\n","    hl_1 = Dense(units=hl_dim, activation='relu', name='Encoder_First_HL')(input)\n","    hl_2 = Dense(units=hl_dim, activation='relu', name='Encoder_Second_HL')(hl_1)\n","\n","    z_mean = Dense(units=latent_dim, activation = activation_mean, name='z_Mean')(hl_2)\n","    z_log_sd = Dense(units=latent_dim, activation = activation_sd, name='z_log_SD')(hl_2)\n","\n","    z = Lambda(sampling, name='z_Sampling_Layer')([z_mean, z_log_sd])\n","\n","    encoder = Model(input, [z_mean, z_log_sd, z], name='Encoder_Model')\n","    return encoder, z_mean, z_log_sd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cGQ01M0pPh9"},"outputs":[],"source":["def create_decoder_model(latent_inputs: KerasTensor, activation_mean: str = None, hl_dim: int = 256,\n","                         original_dim: int = 784) -> Tuple[Model, KerasTensor]:\n","    \"\"\"\n","    Creates a decoder model for generating output based on latent inputs.\n","\n","    Args:\n","        latent_inputs (KerasTensor): The input tensor for the decoder model.\n","        activation_mean (str, optional): Activation function for the output mean layer. Defaults to None.\n","        hl_dim (int, optional): Dimension of the hidden layers. Defaults to 256.\n","        original_dim (int, optional): Dimension of the original data. Defaults to 784.\n","\n","    Returns:\n","        Tuple[Model, tf.Tensor]: A tuple containing the decoder model and the tensor representing the output mean.\n","    \"\"\"\n","\n","    hl_dec1 = Dense(units = hl_dim, activation ='relu', name ='Decoder_First_HL')(latent_inputs)\n","    hl_dec2 = Dense(units = hl_dim, activation ='relu', name ='Decoder_Second_HL')(hl_dec1)\n","\n","    output_mean = Dense(units = original_dim, activation = activation_mean, name ='Output_Mean')(hl_dec2)\n","\n","    decoder = Model(latent_inputs, output_mean, name ='Decoder_Model')\n","\n","    return decoder, output_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_9SyrcVpPh9"},"outputs":[],"source":["def loss_function(original_dim: int, input: KerasTensor, en_decoder_merged: KerasTensor, z_log_sd: KerasTensor, z_mean: KerasTensor) -> KerasTensor:\n","    \"\"\"\n","    Calculates the loss function for a variational autoencoder (VAE).\n","\n","    Args:\n","        original_dim (int): Dimension of the original data.\n","        input (Tensor): The input tensor.\n","        en_decoder_merged (Tensor): The tensor representing the merged output of the encoder and decoder.\n","        z_log_sd (Tensor): The tensor representing the log standard deviation of the latent space.\n","        z_mean (Tensor): The tensor representing the mean of the latent space.\n","\n","    Returns:\n","        Tensor: The calculated ELBO loss.\n","    \"\"\"\n","\n","    r_loss = original_dim * keras.losses.mse(input, en_decoder_merged)\n","\n","    kl_loss =  -0.5 * K.sum(1 + z_log_sd - K.square(z_mean) - K.exp(z_log_sd), axis = 1)\n","\n","    elbo_loss = K.mean(r_loss + kl_loss)\n","\n","    return(elbo_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nobwbo0mpPh9"},"outputs":[],"source":["def run_all(\n","        input: KerasTensor = input,\n","        enc_activation_mean: str = None,\n","        enc_activation_sd: str = None,\n","        hl_dim: int = hl_dim,\n","        latent_dim: int = latent_dim,\n","        original_dim: int = original_dim,\n","        latent_inputs: KerasTensor = latent_inputs,\n","        dec_activation_mean: str = None,\n","        epochs: int = 3,\n","        batch_size: int = 128,\n","        x_val: np.ndarray = x_val,\n","        x_train: np.ndarray = x_train,\n","        learning_rate: float = 0.001\n","    ) -> Tuple[Model, Model, History, Model]:\n","    \"\"\"\n","    Runs the entire pipeline for training a Variational Autoencoder (VAE) model.\n","\n","    Args:\n","        input (KerasTensor): The input tensor for the encoder.\n","        enc_activation_mean (str, optional): Activation function for the mean component of the encoder's latent space layer. Defaults to None = Linear.\n","        enc_activation_sd (str, optional): Activation function for the standard deviation component of the encoder's latent space layer. Defaults to None = Linear.\n","        hl_dim (int, optional): Dimension of the hidden layers. Defaults to hl_dim.\n","        latent_dim (int, optional): Dimension of the latent space. Defaults to latent_dim.\n","        original_dim (int, optional): Dimension of the original data. Defaults to original_dim.\n","        latent_inputs (KerasTensor): The input tensor for the decoder's latent space.\n","        dec_activation_mean (str, optional): Activation function for the mean component of the decoder's output layer. Defaults to None = Linear.\n","        epochs (int, optional): Number of training epochs. Defaults to 3.\n","        batch_size (int, optional): Batch size for training. Defaults to 128.\n","        x_test (ndarray, optional): Test data. Defaults to x_test.\n","        x_train (ndarray, optional): Training data. Defaults to x_train.\n","        learning_rate (float, optional): Learning rate for optimization. Defaults to 0.001.\n","\n","    Returns:\n","        Tuple: A tuple containing the encoder model, decoder model, training history, and VAE model.\n","\n","    Notes:\n","        This function sets up the encoder and decoder models using the specified parameters.\n","        It creates the VAE model by combining the encoder and decoder models.\n","        The VAE model is then trained using the provided data.\n","        The function returns the encoder model, decoder model, training history, and VAE model.\n","    \"\"\"\n","\n","    encoder, z_mean, z_log_sd = create_encoder_model(input, activation_mean = enc_activation_mean, activation_sd = enc_activation_sd, hl_dim = hl_dim, latent_dim = latent_dim)\n","    decoder, output_mean = create_decoder_model(latent_inputs, activation_mean = dec_activation_mean, hl_dim = hl_dim, original_dim = original_dim)\n","\n","    # note: we take z by specifying [2]\n","    en_decoder_merged = decoder(encoder(input)[2])\n","\n","    vae = Model(inputs=input, outputs=en_decoder_merged, name='VAE_Model')\n","\n","    elbo_loss = loss_function(original_dim, input, en_decoder_merged, z_log_sd, z_mean)\n","\n","    vae.add_loss(elbo_loss)\n","    vae.compile(optimizer=keras.optimizers.Adam(learning_rate = learning_rate))\n","\n","    history = vae.fit(x_train, x_train, epochs = epochs, batch_size = batch_size, validation_data = (x_val, x_val), verbose = 0)\n","\n","    return encoder, decoder, history, vae"]},{"cell_type":"markdown","metadata":{"id":"-r44KxVSpPh-"},"source":["### Running the experiments & Plotting"]},{"cell_type":"code","source":["def plot_loss(history: History) -> None:\n","\n","    \"\"\"\n","    Plots the training and validation loss over epochs.\n","\n","    Args:\n","        history (History): The training history object obtained from model training.\n","    \"\"\"\n","\n","    fig, ax = plt.subplots(figsize = (16,9), dpi = 300)\n","    plt.title(label = 'Model Loss by Epoch', loc = 'center')\n","\n","    ax.plot(history.history['loss'], label = 'Training Data', color = 'black')\n","    ax.plot(history.history['val_loss'], label = 'Test Data', color = 'red')\n","    ax.set(xlabel = 'Epoch', ylabel = 'Loss')\n","    plt.xticks(ticks = np.arange(len(history.history['loss']), step = 1), labels = np.arange(1, len(history.history['loss'])+1, step = 1))\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"WAPseBp5vxDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDp22KnGpPh-"},"outputs":[],"source":["encoder, decoder, history, vae = run_all(epochs = 25)\n","plot_loss(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzY5X_HgpPh_"},"outputs":[],"source":["# Exploring when the optimisation converges\n","encoder, decoder, history, vae = run_all(epochs = 100)\n","print(\"Plotting the Latent Representation:\")\n","plot_loss(history)\n","# after about epoch 60, the loss function of the test set does not decrease anymore."]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"orig_nbformat":4,"colab":{"provenance":[],"history_visible":true}},"nbformat":4,"nbformat_minor":0}